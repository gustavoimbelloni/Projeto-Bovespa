#!/usr/bin/env python3
"""
Script de valida√ß√£o e testes espec√≠ficos para o Tech Challenge
Valida cada requisito individualmente e gera relat√≥rio detalhado
"""

import pandas as pd
import os
import json
import logging
from datetime import datetime, date
from pathlib import Path
import unittest
from typing import Dict, List, Any

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TechChallengeValidator:
    """Classe para validar todos os requisitos do Tech Challenge"""
    
    def __init__(self, pipeline_dir: str = "./pipeline_test"):
        """
        Inicializa o validador
        
        Args:
            pipeline_dir: Diret√≥rio com os resultados do pipeline
        """
        self.pipeline_dir = Path(pipeline_dir)
        self.raw_data_dir = self.pipeline_dir / "raw-data"
        self.refined_data_dir = self.pipeline_dir / "refined-data"
        self.logs_dir = self.pipeline_dir / "logs"
        
        self.validation_results = {}
    
    def validate_requirement_1(self) -> Dict[str, Any]:
        """
        Valida Requisito 1: Scrap de dados do site da B3
        
        Returns:
            Resultado da valida√ß√£o
        """
        logger.info("üîç Validando Requisito 1: Scrap de dados da B3")
        
        result = {
            "requirement": "Req 1 - Scrap de dados do site da B3",
            "status": "PASS",
            "details": [],
            "issues": []
        }
        
        try:
            # Verificar se existe script de scrap
            script_path = Path("scripts/b3_scraper.py")
            if script_path.exists():
                result["details"].append("‚úì Script de scrap implementado")
                
                # Verificar se o script cont√©m as funcionalidades necess√°rias
                with open(script_path, 'r') as f:
                    script_content = f.read()
                
                required_features = [
                    ("requests", "Biblioteca requests para HTTP"),
                    ("BeautifulSoup", "Parser HTML BeautifulSoup"),
                    ("parquet", "Suporte a formato Parquet"),
                    ("boto3", "Integra√ß√£o com AWS S3")
                ]
                
                for feature, description in required_features:
                    if feature in script_content:
                        result["details"].append(f"‚úì {description} presente")
                    else:
                        result["issues"].append(f"‚ö† {description} n√£o encontrado")
                
            else:
                result["status"] = "FAIL"
                result["issues"].append("‚ùå Script de scrap n√£o encontrado")
            
            # Verificar se dados foram coletados
            raw_files = list(self.raw_data_dir.glob("**/*.parquet"))
            if raw_files:
                result["details"].append(f"‚úì {len(raw_files)} arquivo(s) de dados brutos encontrado(s)")
                
                # Verificar estrutura dos dados
                df = pd.read_parquet(raw_files[0])
                expected_columns = ['codigo', 'acao', 'tipo', 'qtde_teorica', 'participacao_pct']
                
                for col in expected_columns:
                    if col in df.columns:
                        result["details"].append(f"‚úì Coluna '{col}' presente nos dados")
                    else:
                        result["issues"].append(f"‚ö† Coluna '{col}' ausente nos dados")
                        
            else:
                result["issues"].append("‚ö† Nenhum arquivo de dados brutos encontrado")
                
        except Exception as e:
            result["status"] = "ERROR"
            result["issues"].append(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        return result
    
    def validate_requirement_2(self) -> Dict[str, Any]:
        """
        Valida Requisito 2: Dados brutos no S3 em formato Parquet com parti√ß√£o di√°ria
        
        Returns:
            Resultado da valida√ß√£o
        """
        logger.info("üîç Validando Requisito 2: S3 Parquet com parti√ß√£o di√°ria")
        
        result = {
            "requirement": "Req 2 - S3 Parquet com parti√ß√£o di√°ria",
            "status": "PASS",
            "details": [],
            "issues": []
        }
        
        try:
            # Verificar estrutura de particionamento
            expected_structure = ["year=", "month=", "day="]
            
            for root, dirs, files in os.walk(self.raw_data_dir):
                path_parts = Path(root).parts
                
                # Verificar se cont√©m particionamento por data
                has_year = any("year=" in part for part in path_parts)
                has_month = any("month=" in part for part in path_parts)
                has_day = any("day=" in part for part in path_parts)
                
                if has_year and has_month and has_day:
                    result["details"].append("‚úì Particionamento por year/month/day implementado")
                    break
            else:
                result["issues"].append("‚ö† Particionamento por data n√£o encontrado")
            
            # Verificar arquivos Parquet
            parquet_files = list(self.raw_data_dir.glob("**/*.parquet"))
            if parquet_files:
                result["details"].append(f"‚úì {len(parquet_files)} arquivo(s) Parquet encontrado(s)")
                
                # Verificar se √© poss√≠vel ler os arquivos
                for file in parquet_files:
                    try:
                        df = pd.read_parquet(file)
                        result["details"].append(f"‚úì Arquivo {file.name} leg√≠vel ({len(df)} registros)")
                    except Exception as e:
                        result["issues"].append(f"‚ùå Erro ao ler {file.name}: {str(e)}")
                        
            else:
                result["status"] = "FAIL"
                result["issues"].append("‚ùå Nenhum arquivo Parquet encontrado")
                
        except Exception as e:
            result["status"] = "ERROR"
            result["issues"].append(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        return result
    
    def validate_requirement_3_4(self) -> Dict[str, Any]:
        """
        Valida Requisitos 3 e 4: Lambda trigger e linguagem
        
        Returns:
            Resultado da valida√ß√£o
        """
        logger.info("üîç Validando Requisitos 3 e 4: Lambda trigger")
        
        result = {
            "requirement": "Req 3-4 - Lambda trigger para Glue",
            "status": "PASS",
            "details": [],
            "issues": []
        }
        
        try:
            # Verificar se existe fun√ß√£o Lambda
            lambda_path = Path("lambda/trigger_glue_job.py")
            if lambda_path.exists():
                result["details"].append("‚úì Fun√ß√£o Lambda implementada")
                
                with open(lambda_path, 'r') as f:
                    lambda_content = f.read()
                
                # Verificar funcionalidades da Lambda
                required_features = [
                    ("boto3", "Cliente AWS SDK"),
                    ("glue", "Integra√ß√£o com AWS Glue"),
                    ("start_job_run", "Inicializa√ß√£o de job Glue"),
                    ("Records", "Processamento de eventos S3")
                ]
                
                for feature, description in required_features:
                    if feature in lambda_content:
                        result["details"].append(f"‚úì {description} implementado")
                    else:
                        result["issues"].append(f"‚ö† {description} n√£o encontrado")
                
                # Verificar se √© Python (Requisito 4)
                if "python" in lambda_content.lower() or lambda_path.suffix == ".py":
                    result["details"].append("‚úì Linguagem Python utilizada")
                
            else:
                result["status"] = "FAIL"
                result["issues"].append("‚ùå Fun√ß√£o Lambda n√£o encontrada")
            
            # Verificar simula√ß√£o de evento S3
            event_file = self.logs_dir / "s3_event_simulation.json"
            if event_file.exists():
                result["details"].append("‚úì Evento S3 simulado")
                
                with open(event_file, 'r') as f:
                    event_data = json.load(f)
                
                if "Records" in event_data and event_data["Records"]:
                    result["details"].append("‚úì Estrutura de evento S3 v√°lida")
                else:
                    result["issues"].append("‚ö† Estrutura de evento S3 inv√°lida")
                    
            else:
                result["issues"].append("‚ö† Simula√ß√£o de evento S3 n√£o encontrada")
                
        except Exception as e:
            result["status"] = "ERROR"
            result["issues"].append(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        return result
    
    def validate_requirement_5(self) -> Dict[str, Any]:
        """
        Valida Requisito 5: Job Glue visual com transforma√ß√µes
        
        Returns:
            Resultado da valida√ß√£o
        """
        logger.info("üîç Validando Requisito 5: Job Glue com transforma√ß√µes")
        
        result = {
            "requirement": "Req 5 - Job Glue visual com transforma√ß√µes",
            "status": "PASS",
            "details": [],
            "issues": []
        }
        
        try:
            # Verificar configura√ß√£o do job Glue
            glue_config = Path("glue-jobs/glue_job_config.json")
            if glue_config.exists():
                result["details"].append("‚úì Configura√ß√£o do job Glue encontrada")
                
                with open(glue_config, 'r') as f:
                    config = json.load(f)
                
                # Verificar modo visual
                if config.get("jobMode") == "VISUAL":
                    result["details"].append("‚úì Job configurado em modo visual")
                else:
                    result["issues"].append("‚ö† Job n√£o est√° em modo visual")
                
            else:
                result["issues"].append("‚ö† Configura√ß√£o do job Glue n√£o encontrada")
            
            # Verificar dados refinados para validar transforma√ß√µes
            refined_files = list(self.refined_data_dir.glob("**/*.parquet"))
            if refined_files:
                result["details"].append(f"‚úì {len(refined_files)} arquivo(s) refinado(s) encontrado(s)")
                
                # Carregar dados para verificar transforma√ß√µes
                dfs = [pd.read_parquet(f) for f in refined_files]
                combined_df = pd.concat(dfs, ignore_index=True)
                
                # Verificar Transforma√ß√£o A: Agrupamento/Sumariza√ß√£o
                if 'total_quantidade_teorica' in combined_df.columns:
                    result["details"].append("‚úì Transforma√ß√£o A: Sumariza√ß√£o implementada")
                else:
                    result["issues"].append("‚ùå Transforma√ß√£o A: Sumariza√ß√£o n√£o encontrada")
                
                # Verificar Transforma√ß√£o B: Renomea√ß√£o de colunas
                renamed_columns = ['quantidade_teorica_acoes', 'percentual_participacao_ibov']
                found_renamed = [col for col in renamed_columns if col in combined_df.columns]
                
                if len(found_renamed) >= 2:
                    result["details"].append(f"‚úì Transforma√ß√£o B: {len(found_renamed)} colunas renomeadas")
                else:
                    result["issues"].append(f"‚ùå Transforma√ß√£o B: Apenas {len(found_renamed)} colunas renomeadas (necess√°rio 2)")
                
                # Verificar Transforma√ß√£o C: C√°lculo com data
                date_columns = [col for col in combined_df.columns if 'dia' in col.lower() or 'date' in col.lower()]
                if date_columns:
                    result["details"].append("‚úì Transforma√ß√£o C: C√°lculo com data implementado")
                else:
                    result["issues"].append("‚ùå Transforma√ß√£o C: C√°lculo com data n√£o encontrado")
                
            else:
                result["status"] = "FAIL"
                result["issues"].append("‚ùå Nenhum arquivo refinado encontrado")
                
        except Exception as e:
            result["status"] = "ERROR"
            result["issues"].append(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        return result
    
    def validate_requirement_6(self) -> Dict[str, Any]:
        """
        Valida Requisito 6: Dados refinados com particionamento
        
        Returns:
            Resultado da valida√ß√£o
        """
        logger.info("üîç Validando Requisito 6: Dados refinados particionados")
        
        result = {
            "requirement": "Req 6 - Dados refinados particionados",
            "status": "PASS",
            "details": [],
            "issues": []
        }
        
        try:
            # Verificar estrutura de particionamento refinado
            refined_structure_found = False
            
            for root, dirs, files in os.walk(self.refined_data_dir):
                path_parts = Path(root).parts
                
                # Verificar particionamento por data e a√ß√£o
                has_year = any("year=" in part for part in path_parts)
                has_month = any("month=" in part for part in path_parts)
                has_day = any("day=" in part for part in path_parts)
                has_action = any("tipo=" in part for part in path_parts)
                
                if has_year and has_month and has_day and has_action:
                    refined_structure_found = True
                    result["details"].append("‚úì Particionamento por data e tipo de a√ß√£o implementado")
                    break
            
            if not refined_structure_found:
                result["issues"].append("‚ùå Particionamento duplo (data + a√ß√£o) n√£o encontrado")
            
            # Verificar pasta refined
            if "refined" in str(self.refined_data_dir):
                result["details"].append("‚úì Pasta 'refined' utilizada")
            else:
                result["issues"].append("‚ö† Pasta 'refined' n√£o identificada")
            
            # Verificar formato Parquet nos dados refinados
            refined_parquet = list(self.refined_data_dir.glob("**/*.parquet"))
            if refined_parquet:
                result["details"].append(f"‚úì {len(refined_parquet)} arquivo(s) Parquet refinado(s)")
            else:
                result["status"] = "FAIL"
                result["issues"].append("‚ùå Nenhum arquivo Parquet refinado encontrado")
                
        except Exception as e:
            result["status"] = "ERROR"
            result["issues"].append(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        return result
    
    def validate_requirement_7(self) -> Dict[str, Any]:
        """
        Valida Requisito 7: Cataloga√ß√£o autom√°tica no Glue Catalog
        
        Returns:
            Resultado da valida√ß√£o
        """
        logger.info("üîç Validando Requisito 7: Glue Data Catalog")
        
        result = {
            "requirement": "Req 7 - Glue Data Catalog",
            "status": "PASS",
            "details": [],
            "issues": []
        }
        
        try:
            # Verificar simula√ß√£o do cat√°logo
            catalog_file = self.logs_dir / "glue_catalog_simulation.json"
            if catalog_file.exists():
                result["details"].append("‚úì Configura√ß√£o do Data Catalog simulada")
                
                with open(catalog_file, 'r') as f:
                    catalog_data = json.load(f)
                
                # Verificar database default
                if catalog_data.get("database") == "default" or "default" in str(catalog_data.get("database", "")):
                    result["details"].append("‚úì Database 'default' configurado")
                else:
                    result["issues"].append("‚ö† Database 'default' n√£o configurado")
                
                # Verificar tabela
                if "table" in catalog_data:
                    result["details"].append(f"‚úì Tabela '{catalog_data.get('table')}' configurada")
                else:
                    result["issues"].append("‚ö† Configura√ß√£o de tabela n√£o encontrada")
                
                # Verificar schema
                if "schema" in catalog_data:
                    result["details"].append("‚úì Schema da tabela definido")
                else:
                    result["issues"].append("‚ö† Schema da tabela n√£o definido")
                
            else:
                result["issues"].append("‚ö† Configura√ß√£o do Data Catalog n√£o encontrada")
            
            # Verificar configura√ß√£o no job Glue
            glue_config = Path("glue-jobs/glue_job_config.json")
            if glue_config.exists():
                with open(glue_config, 'r') as f:
                    config = json.load(f)
                
                # Procurar por configura√ß√£o de cataloga√ß√£o
                dag_str = json.dumps(config.get("dag", {}))
                if "enableUpdateCatalog" in dag_str or "UPDATE_IN_DATABASE" in dag_str:
                    result["details"].append("‚úì Cataloga√ß√£o autom√°tica configurada no job Glue")
                else:
                    result["issues"].append("‚ö† Cataloga√ß√£o autom√°tica n√£o configurada")
                    
        except Exception as e:
            result["status"] = "ERROR"
            result["issues"].append(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        return result
    
    def validate_requirement_8(self) -> Dict[str, Any]:
        """
        Valida Requisito 8: Dados dispon√≠veis no Athena
        
        Returns:
            Resultado da valida√ß√£o
        """
        logger.info("üîç Validando Requisito 8: Disponibilidade no Athena")
        
        result = {
            "requirement": "Req 8 - Dados dispon√≠veis no Athena",
            "status": "PASS",
            "details": [],
            "issues": []
        }
        
        try:
            # Verificar simula√ß√£o de consultas Athena
            athena_file = self.logs_dir / "athena_queries_simulation.json"
            if athena_file.exists():
                result["details"].append("‚úì Consultas Athena simuladas")
                
                with open(athena_file, 'r') as f:
                    queries_data = json.load(f)
                
                # Verificar tipos de consulta
                query_types = list(queries_data.keys())
                result["details"].append(f"‚úì {len(query_types)} tipo(s) de consulta testado(s)")
                
                # Verificar se h√° resultados
                for query_type, query_info in queries_data.items():
                    if "resultado" in query_info and query_info["resultado"]:
                        result["details"].append(f"‚úì Consulta '{query_type}' retornou dados")
                    else:
                        result["issues"].append(f"‚ö† Consulta '{query_type}' sem resultados")
                
            else:
                result["issues"].append("‚ö† Simula√ß√£o de consultas Athena n√£o encontrada")
            
            # Verificar se dados refinados s√£o leg√≠veis (simulando Athena)
            refined_files = list(self.refined_data_dir.glob("**/*.parquet"))
            readable_files = 0
            
            for file in refined_files:
                try:
                    df = pd.read_parquet(file)
                    if len(df) > 0:
                        readable_files += 1
                except:
                    pass
            
            if readable_files > 0:
                result["details"].append(f"‚úì {readable_files} arquivo(s) leg√≠vel(is) para consulta")
            else:
                result["status"] = "FAIL"
                result["issues"].append("‚ùå Nenhum arquivo leg√≠vel encontrado")
                
        except Exception as e:
            result["status"] = "ERROR"
            result["issues"].append(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        return result
    
    def run_full_validation(self) -> Dict[str, Any]:
        """
        Executa valida√ß√£o completa de todos os requisitos
        
        Returns:
            Relat√≥rio completo de valida√ß√£o
        """
        logger.info("üîç INICIANDO VALIDA√á√ÉO COMPLETA DOS REQUISITOS")
        logger.info("=" * 60)
        
        validators = [
            self.validate_requirement_1,
            self.validate_requirement_2,
            self.validate_requirement_3_4,
            self.validate_requirement_5,
            self.validate_requirement_6,
            self.validate_requirement_7,
            self.validate_requirement_8
        ]
        
        validation_results = []
        passed_count = 0
        
        for validator in validators:
            try:
                result = validator()
                validation_results.append(result)
                
                # Log resultado
                status_emoji = "‚úÖ" if result["status"] == "PASS" else "‚ùå" if result["status"] == "FAIL" else "‚ö†Ô∏è"
                logger.info(f"{status_emoji} {result['requirement']}: {result['status']}")
                
                if result["status"] == "PASS":
                    passed_count += 1
                
                # Log detalhes
                for detail in result["details"]:
                    logger.info(f"    {detail}")
                
                # Log issues
                for issue in result["issues"]:
                    logger.warning(f"    {issue}")
                
            except Exception as e:
                logger.error(f"‚ùå Erro na valida√ß√£o: {str(e)}")
        
        # Relat√≥rio final
        total_requirements = len(validation_results)
        success_rate = (passed_count / total_requirements) * 100 if total_requirements > 0 else 0
        
        final_report = {
            "validation_summary": {
                "timestamp": datetime.now().isoformat(),
                "total_requirements": total_requirements,
                "passed": passed_count,
                "failed": total_requirements - passed_count,
                "success_rate": round(success_rate, 2)
            },
            "detailed_results": validation_results,
            "overall_status": "PASS" if passed_count == total_requirements else "PARTIAL" if passed_count > 0 else "FAIL"
        }
        
        # Salvar relat√≥rio
        report_file = self.logs_dir / "validation_report.json"
        with open(report_file, 'w') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        logger.info("=" * 60)
        logger.info(f"üèÅ VALIDA√á√ÉO CONCLU√çDA: {passed_count}/{total_requirements} requisitos aprovados")
        logger.info(f"üìä Taxa de sucesso: {success_rate:.1f}%")
        logger.info(f"üìÑ Relat√≥rio salvo em: {report_file}")
        
        return final_report

def main():
    """Fun√ß√£o principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Valida√ß√£o dos requisitos do Tech Challenge')
    parser.add_argument('--pipeline-dir', default='./pipeline_test', help='Diret√≥rio do pipeline')
    
    args = parser.parse_args()
    
    # Executar valida√ß√£o
    validator = TechChallengeValidator(pipeline_dir=args.pipeline_dir)
    report = validator.run_full_validation()
    
    # Exit code baseado no resultado
    if report["overall_status"] == "PASS":
        exit(0)
    elif report["overall_status"] == "PARTIAL":
        exit(1)
    else:
        exit(2)

if __name__ == "__main__":
    main()

